{"paragraphs":[{"text":"%pyspark\n\nimport random\nimport numpy as np\nimport tensorflow as tf","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228008_-553192580","id":"20170510-211716_1529885855","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3653"},{"text":"%pyspark\n\n# Turn data into a Numpy array. Note that the np_rdd from the last notebook SHOULD be here.\ndata = np.array(np_rdd)","dateUpdated":"2017-05-10T22:19:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228018_-543189109","id":"20170510-213225_560429339","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3654"},{"text":"%pyspark\n\n# Helper function to generate multiple random parameters.\ndef random_range(start_point, end_point, amount):\n    return [random.randint(start_point, end_point) for a in range(amount)]","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228018_-543189109","id":"20170510-211731_358669442","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3655"},{"text":"%pyspark\n\n# ========= Settings\nLEARNING_RATE_START = 0.01\nLEARNING_RATE_END = 0.01\n\nEPOCHS_START = 1\nEPOCHS_END = 1\n\nBATCH_SIZE_START = 2048\nBATCH_SIZE_END = 4096\n\nHIDDEN_LAYER_SIZE_START = 128\nHIDDEN_LAYER_SIZE_END = 256\n\nCOMBINATION_AMOUNT = 2\n\n# ========= Create ranges\nLEARNING_RATE = 0.01\nEPOCHS = random_range(EPOCHS_START, EPOCHS_END, COMBINATION_AMOUNT)\nBATCH_SIZE = random_range(BATCH_SIZE_START, BATCH_SIZE_END, COMBINATION_AMOUNT)\nHIDDEN_LAYER_SIZE = random_range(HIDDEN_LAYER_SIZE_START, HIDDEN_LAYER_SIZE_END, COMBINATION_AMOUNT)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228019_-543573858","id":"20170510-211920_1842964316","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3656"},{"text":"%pyspark\n\n# Function to add data as a value to each row of the RDD.\ndef add_data(data):\n    def add_data_fn(params):\n        params[\"data\"] = data\n        return params\n    return add_data_fn","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228019_-543573858","id":"20170510-212057_1132185716","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3657"},{"text":"%pyspark\n\n# Create a list of dictionaries that hold all parameters.\nall_parameters = []\n\nfor i in range(COMBINATION_AMOUNT):\n    parameters = {\n        \"learning_rate\": LEARNING_RATE,\n        \"epochs\": EPOCHS[i],\n        \"batch_size\": BATCH_SIZE[i],\n        \"hidden_layer_size\": HIDDEN_LAYER_SIZE[i]\n    }\n    all_parameters.append(parameters)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228019_-543573858","id":"20170510-212108_1186699505","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3658"},{"text":"%pyspark\n\n# Create an RDD out of all parameters.\nmodel_parameter_rdd = sc.parallelize(all_parameters)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228020_-545497602","id":"20170510-212241_1184797607","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3659"},{"text":"%pyspark\n\n# Create an RDD with parameters and data.\nmodel_rdd = model_parameter_rdd.map(add_data(data))","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228020_-545497602","id":"20170510-212303_978472977","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3660"},{"text":"%pyspark\n\ndef validation_and_model(everything):\n    \n    # === Helper function.\n    def get_data_by_customer(data,customer_name_column):\n        customers = list(set(customer_name_column))\n    \n        all_customers_receipts = []\n        for customer in customers:\n            customer_receipt_indexes = np.where(customer_name_column==customer)\n            customer_receipts = data[customer_receipt_indexes]\n            all_customers_receipts.append(customer_receipts)\n    \n        return all_customers_receipts\n        \n    # ===  Helper function.\n    def get_windowed_data_by_customer(data,customer_name_column,window_size = 1, flatten = True):\n        customers = list(set(customer_name_column))\n        print \"num of customers \", len(customers)\n        all_customers_inputs = []\n        all_customers_targets = []\n        for customer in customers:\n            customer_input, customer_targets = receipt_window(data,customer,window_size,flatten)\n            all_customers_inputs.extend(customer_input)\n            all_customers_targets.extend(customer_targets)\n    \n        all_customers_inputs = np.array(all_customers_inputs,dtype=np.int16)\n        all_customers_targets = np.array(all_customers_targets,dtype=np.int16)\n        return all_customers_inputs,all_customers_targets\n        \n    # === Helper function.\n    def receipt_window(data,given_customer_id,window_size = 1,flatten=True):\n        customer_name_index = 0\n        # Organize data\n        all_customer_data = data[data[:,customer_name_index] == given_customer_id]\n    \n        customer_inputs=[]\n        customer_targets=[]\n        num_games = len(all_customer_data)\n        for window_location in range(0,num_games-window_size):\n            inputs = all_customer_data[window_location:window_location+window_size,2:]\n            targets = all_customer_data[window_location+window_size,2:]\n    \n            num_targets = targets[0]\n            #offset by 1 since first entry is length of targets\n            for target_id in range(1,num_targets+1):\n                target = targets[target_id]\n                if flatten:\n                    inputs = np.array(inputs).flatten()\n                customer_inputs.append(inputs)\n                customer_targets.append(target)\n    \n        return customer_inputs, customer_targets\n\n    # === The actual model!\n    def model(everything):\n        import tensorflow as tf\n        import math\n        all_inputs = everything['inputs']\n        all_targets = everything['targets']\n        learning_rate = everything['learning_rate']\n        batch_size = everything['batch_size']\n        embedding_size = everything['hidden_layer_size']\n        num_epochs = everything['epochs']\n    \n    \n        # Extract the lengths from the inputs\n        all_lengths = all_inputs[:,0]\n        # Extract the items (remove the lengths)\n        all_inputs = all_inputs[:,1:]\n    \n        # We're going to use the lengths to do the average of items later, so for convenience we take the inverse\n        all_lengths = 1.0/all_lengths\n    \n        # we need to fix the shape for our graph\n        all_lengths = np.expand_dims(all_lengths,1)\n        all_targets = np.expand_dims(all_targets,1)\n    \n        # We fetch the maximum size of items in a receipt\n        context_size = all_inputs.shape[1]\n    \n        # What's the max value of the item indexes?\n        vocabulary_size = all_inputs.max() + 1 # plus 1 to account for the fact that 0 isn't an item\n    \n        # number of samples for candidate sampling approximation\n        num_sampled = 100\n    \n        graph = tf.Graph()\n    \n        with graph.as_default():\n            # Input data.\n            train_inputs = tf.placeholder(tf.int32, shape=[None, context_size],name=\"inputs\")\n            train_labels = tf.placeholder(tf.int32, shape=[None,1 ],name=\"targets\")\n            basket_size = tf.placeholder(tf.float32, shape=[None,1],name=\"lengths\")\n    \n            # Look up embeddings for inputs.\n            embedding = tf.Variable(\n                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0,dtype=tf.float32),name=\"embeddings\")\n    \n            embed = tf.nn.embedding_lookup(embedding, train_inputs,\"lookedup_embedding\")\n    \n            # take mean of embeddings of context words for context embedding\n            embed_context = basket_size*tf.reduce_sum(embed, 1,name=\"embed_average_context\")\n    \n            # Construct the variables for the NCE loss\n            nce_weights = tf.Variable(\n                tf.truncated_normal([vocabulary_size, embedding_size],\n                                    stddev=1.0 / math.sqrt(embedding_size)),name=\"nce_weights\")\n            nce_biases = tf.Variable(tf.zeros([vocabulary_size]),name='nce_biases')\n    \n            outputs = tf.matmul(embed_context,nce_weights,transpose_b=True) + nce_biases\n    \n            SM_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(train_labels),logits=outputs))\n    \n            # Construct the SGD optimizer\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate,name=\"optimizer\").minimize(SM_loss)\n    \n            # Add variable initializer.\n            init = tf.initialize_all_variables()\n    \n        # Step 5: Begin training.\n        with tf.Session(graph=graph) as session:\n            # We must initialize all variables before we use them.\n            init.run()\n            print(\"Initialized\")\n    \n            # Use 80% for training 20% for validation\n            # Chop up the dataset (including inputs/targets and lengths for input)\n            valid_start = int(len(all_targets)*0.8)\n            valid_inputs = all_inputs[valid_start:]\n            valid_targets = all_targets[valid_start:]\n            valid_lengths = all_lengths[valid_start:]\n    \n            all_inputs = all_inputs[:valid_start]\n            all_targets = all_targets[:valid_start]\n            all_lengths = all_lengths[:valid_start]\n    \n            # Calculate how many batches we have for the training data and validation data\n            num_batches = int(1+ (len(all_targets)/batch_size))\n            num_val_batches = int(1+ (len(valid_targets)/batch_size))\n            print \"num training batches: \", num_batches\n            print \"num validation batches: \", num_val_batches\n    \n            # Graph writer for tensorboard\n            train_writer = tf.summary.FileWriter('train',graph)\n    \n            # loop over num_epochs\n            for epoch in range(num_epochs):\n    \n                ##################\n                # Training optimization\n                ##################\n                average_loss = 0\n                # Loop over each batch\n                for batch in range(num_batches):\n                    start = batch * batch_size\n                    end = min((batch+1) * batch_size,valid_start)\n    \n                    batch_inputs = all_inputs[start:end]\n                    batch_labels = all_targets[start:end]\n                    batch_lengths = all_lengths[start:end]\n                    # Pass the inputs, labels, and lengths for the inputs\n                    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels,basket_size:batch_lengths}\n    \n                    # We perform one update step by evaluating the optimizer op\n                    session.run(optimizer, feed_dict=feed_dict)\n                    print batch\n    \n                ##################\n                # Validation error calculation\n                ##################\n                average_val_loss = 0\n                for batch in range(num_val_batches):\n                    start = batch * batch_size\n                    end = min((batch+1) * batch_size,len(valid_targets))\n    \n                    batch_inputs = valid_inputs[start:end]\n                    batch_labels = valid_targets[start:end]\n                    batch_lengths = valid_lengths[start:end]\n    \n                    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels,basket_size:batch_lengths}\n    \n                    # We use the softmax loss to get a more accurate picture of the loss\n                    loss_val = session.run(SM_loss, feed_dict=feed_dict)\n                    average_val_loss += loss_val\n    \n                print(\"Average loss at step \", epoch, \": \", average_loss/num_batches, \"validation\",average_val_loss/num_val_batches)\n    \n            # Remove the data from the dictionary\n            del everything['inputs']\n            del everything['targets']\n            # Calculate the average validation score and add it to the dictionary, then return it\n            score = average_val_loss/num_val_batches\n            everything['score'] = score\n            return everything\n\n    # === Main part of the function.\n\n    # Get the data out of the dictionary\n    data = everything['data']\n\n    # Remove it from the dictionary\n    del everything['data']\n\n    # Names of columns (for convenience)\n    column_names = [\"Customer_ID\",\"Date\",\"Receipt_len\",\"Items\"]\n\n    # Get the index of the column containing the customer id\n    customer_name_index = column_names.index(\"Customer_ID\")\n\n    print \"Data shape before processing\", data.shape\n\n    # Get the customer id column\n    customer_name_column = data[:,customer_name_index]\n\n    # Window the data and separate into inputs and targets\n    all_inputs, all_targets = get_windowed_data_by_customer(data,customer_name_column)\n\n    # Add the inputs and targets to the dictionary and run the model with it\n    everything['inputs'] = all_inputs\n    everything['targets'] = all_targets\n    return model(everything)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228020_-545497602","id":"20170510-212331_2086393606","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3661"},{"text":"%pyspark\n\nresults = model_rdd.map(validation_and_model)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228021_-545882351","id":"20170510-212723_1210716079","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3662"},{"text":"%pyspark\n\n# But first, the naive model for a bench mark.\ndef validation_and_naive_model(everything):\n    \n    # === Helper function.\n    def get_data_by_customer(data,customer_name_column):\n        customers = list(set(customer_name_column))\n    \n        all_customers_receipts = []\n        for customer in customers:\n            customer_receipt_indexes = np.where(customer_name_column==customer)\n            customer_receipts = data[customer_receipt_indexes]\n            all_customers_receipts.append(customer_receipts)\n    \n        return all_customers_receipts\n        \n    # ===  Helper function.\n    def get_windowed_data_by_customer(data,customer_name_column,window_size = 1, flatten = True):\n        customers = list(set(customer_name_column))\n        print \"num of customers \", len(customers)\n        all_customers_inputs = []\n        all_customers_targets = []\n        for customer in customers:\n            customer_input, customer_targets = receipt_window(data,customer,window_size,flatten)\n            all_customers_inputs.extend(customer_input)\n            all_customers_targets.extend(customer_targets)\n    \n        all_customers_inputs = np.array(all_customers_inputs,dtype=np.int16)\n        all_customers_targets = np.array(all_customers_targets,dtype=np.int16)\n        return all_customers_inputs,all_customers_targets\n        \n    # === Helper function.\n    def receipt_window(data,given_customer_id,window_size = 1,flatten=True):\n        customer_name_index = 0\n        # Organize data\n        all_customer_data = data[data[:,customer_name_index] == given_customer_id]\n    \n        customer_inputs=[]\n        customer_targets=[]\n        num_games = len(all_customer_data)\n        for window_location in range(0,num_games-window_size):\n            inputs = all_customer_data[window_location:window_location+window_size,2:]\n            targets = all_customer_data[window_location+window_size,2:]\n    \n            num_targets = targets[0]\n            #offset by 1 since first entry is length of targets\n            for target_id in range(1,num_targets+1):\n                target = targets[target_id]\n                if flatten:\n                    inputs = np.array(inputs).flatten()\n                customer_inputs.append(inputs)\n                customer_targets.append(target)\n    \n        return customer_inputs, customer_targets\n\n    # === The actual model!\n    def naive_model(everything):\n        all_targets = everything['targets']\n        batch_size = everything['batch_size']\n    \n        all_targets = np.expand_dims(all_targets,1)\n    \n        # What's the max value of the item indexes?\n        vocabulary_size = 10000 + 1 # plus 1 to account for the fact that 0 isn't an item\n    \n        # number of samples for candidate sampling approximation\n        num_sampled = 100\n    \n        graph = tf.Graph()\n    \n        with graph.as_default():\n            # Input data.\n            train_labels = tf.placeholder(tf.int32, shape=[None,1 ],name=\"targets\")\n            naive_prediction = tf.placeholder(tf.float32, shape=[1,vocabulary_size],name=\"naive_prediction\")\n    \n            naive_prediction_tiled = tf.tile(naive_prediction,tf.shape(train_labels))\n    \n            naive_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(train_labels),logits=naive_prediction_tiled))\n    \n            # Add variable initializer.\n            init = tf.initialize_all_variables()\n    \n        # Step 5: Begin training.\n        with tf.Session(graph=graph) as session:\n            # We must initialize all variables before we use them.\n            init.run()\n            print(\"Initialized\")\n    \n            # Use 80% for training 20% for validation\n            # Chop up the dataset (including inputs/targets and lengths for input)\n            valid_start = int(len(all_targets)*0.8)\n            valid_targets = all_targets[valid_start:]\n    \n            all_targets = all_targets[:valid_start]\n    \n            # Calculate how many batches we have for the training data and validation data\n            num_val_batches = int(1+ (len(valid_targets)/batch_size))\n            print \"num validation batches: \", num_val_batches\n    \n            # Calculate a naive solution\n            from collections import Counter\n    \n            # Count how many times each item occurs\n            counts = Counter(all_targets.squeeze())\n    \n            # Create an empty array to fill\n            naive_numpy = np.zeros(vocabulary_size,dtype=np.float32)\n    \n            # For each item we calculate the average occurrance of the item\n            for item in counts:\n                naive_numpy[item] = counts[item]/float(all_targets.shape[0])\n            naive_numpy = np.expand_dims(naive_numpy,0)\n    \n            ##################\n            # Naive error calculation first\n            ##################\n            # initialize value for loss\n            average_val_loss = 0\n            for batch in range(num_val_batches):\n    \n                # find start and end of this batch\n                start = batch * batch_size\n                end = min((batch+1) * batch_size,len(valid_targets))\n    \n                # Get the batch labels\n                batch_labels = valid_targets[start:end]\n    \n                # Create a dictionary to feed the inputs/labels to run the model on\n                feed_dict = {naive_prediction: naive_numpy, train_labels: batch_labels}\n    \n                # Run the naive error operation\n                loss_val = session.run(naive_loss, feed_dict=feed_dict)\n                # Accumulate the error\n                average_val_loss += loss_val\n            # Average the accumulated error\n            print(\"Naive error \", average_val_loss/num_val_batches)\n    \n            # Remove the data from the dictionary\n            del everything['inputs']\n            del everything['targets']\n            # Calculate the average validation score and add it to the dictionary, then return it\n            score = average_val_loss/num_val_batches\n            everything['score'] = score\n            return everything\n\n    # === Main part of the function.\n\n    # Get the data out of the dictionary\n    data = everything['data']\n\n    # Remove it from the dictionary\n    del everything['data']\n\n    # Names of columns (for convenience)\n    column_names = [\"Customer_ID\",\"Date\",\"Receipt_len\",\"Items\"]\n\n    # Get the index of the column containing the customer id\n    customer_name_index = column_names.index(\"Customer_ID\")\n\n    print \"Data shape before processing\", data.shape\n\n    # Get the customer id column\n    customer_name_column = data[:,customer_name_index]\n\n    # Window the data and separate into inputs and targets\n    all_inputs, all_targets = get_windowed_data_by_customer(data,customer_name_column)\n\n    # Add the inputs and targets to the dictionary and run the model with it\n    everything['inputs'] = all_inputs\n    everything['targets'] = all_targets\n    return naive_model(everything)","dateUpdated":"2017-05-10T22:10:28+0000","config":{"lineNumbers":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228021_-545882351","id":"20170510-213617_1254951138","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3663"},{"text":"%pyspark\n\n# Here we actually run the naive model.\nnaive_params = {\n    \"data\": data,\n    \"batch_size\": BATCH_SIZE[0]\n}\n\nv = validation_and_naive_model(naive_params)\nprint v","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228022_-544728104","id":"20170510-214208_296443955","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3664"},{"text":"%pyspark\n\nresult_list = results.collect()\n\nfor i in result_list:\n    print i","dateUpdated":"2017-05-10T22:10:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228022_-544728104","id":"20170510-213016_1984256417","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3665"},{"text":"%pyspark\n\n","dateUpdated":"2017-05-10T22:11:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1494454228022_-544728104","id":"20170510-213142_628923497","dateCreated":"2017-05-10T22:10:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3666"}],"name":"Session 4: Retail Prediction","id":"2CG8DH7PU","angularObjects":{"2CF5TWB17:shared_process":[],"2CFXF5HM8:shared_process":[],"2CHAU4NEC:shared_process":[],"2CF7PJD9N:shared_process":[],"2CJFMG4KD:shared_process":[],"2CHRHK4MK:shared_process":[],"2CGY8RSQ1:shared_process":[],"2CH4KHBVF:shared_process":[],"2CJNEZXJX:shared_process":[],"2CEVK558U:shared_process":[],"2CG8DH38U:shared_process":[],"2CF7D4BXP:shared_process":[],"2CHXG4HDQ:shared_process":[],"2CJSKJNRF:shared_process":[],"2CHH4VU1J:shared_process":[],"2CG2MNY49:shared_process":[],"2CHH5NJJY:shared_process":[],"2CFBWEPA4:shared_process":[],"2CFEPKHXX:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}